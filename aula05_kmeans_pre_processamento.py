# -*- coding: utf-8 -*-
"""Aula05_KMeans_pre_processamento.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sxGGnq8tFYcLN23GZb4fAxVfs0gQnv6r
"""

import pandas as pd
import numpy as np

def normalizar(x): #x será uma lista de valores numéricos
  return (x - np.min(x))/(np.max(x) - np.min(x))

def padronizar(x):
  return (x -np.mean(x))/np.std(x)

df = pd.read_csv("iris.data", names=['sepal_length','sepal_width',
                                     'petal_length','petal_width',
                                     'class'])
df

import seaborn as sb
sb.pairplot(df, hue='class')

from scipy.stats import shapiro
# Verificar normalidade usando o teste de Shapiro-Wilk
def verifica_normalidade(dataframe, coluna):
    coluna_data = dataframe[coluna]
    # Realizar o teste de Shapiro-Wilk
    statistic, p_valor = shapiro(coluna_data)
    # Definir o nível de significância
    nivel_significancia = 0.1
    # Verificar se a hipótese nula de normalidade pode ser rejeitada
    if p_valor > nivel_significancia:
        print(f"A coluna '{coluna}' segue uma distribuição normal")
        return True
    else:
        print(f"A coluna '{coluna}' não segue uma distribuição normal")
        return False

entradas = df.drop('class', axis=1)
entradas.head(1)

entradas.columns.to_list()

saida = df['class']
saida.head(1)

for coluna in entradas.columns.to_list():
  if verifica_normalidade(entradas, coluna):
    entradas[coluna] = padronizar(entradas[coluna])#se for distribuição normal, padroniza
  else:#senão, normaliza
    entradas[coluna] = normalizar(entradas[coluna])

entradas

print("Coluna       |Máx  |   Min")
for colum in entradas.columns.to_list():
  print(colum,"|", entradas[colum].max(),"|", entradas[colum].min() )
  print("____________________________")

#sepal_length   sepal_width   petal_length   petal_width
#[   5.1,        3.5,             1.4,         0.2,          Iris-setosa ]
compr_sepal = (5.1 - 4.3) / (7.9 - 4.3)
larg_sepal =  (3.5 - 2.0) / (4.4 - 2.0)
compr_petal = (1.4 - 1.0) / (6.9 - 1.0)
larg_petal =  (0.2 - 0.1) / (2.5 - 0.1)
x = [compr_sepal, larg_sepal, compr_petal, larg_petal]
x

!pip install skimpy
from skimpy import skim
skim(entradas)

df['class'] = df['class'].replace({"Iris-setosa": 0,
                                   "Iris-versicolor": 1,
                                   "Iris-virginica":2}
                                  )

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import numpy as np

dados = np.array(entradas)
kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(dados)

kmeans.labels_

exemplo = np.array(entradas.iloc[[-1]])
previsao = kmeans.predict(exemplo)
print(f"Previsão {previsao} Valor real: {df['class'].iloc[-1]}")

def soma_quadrados_intra_clusters(dataset):
  wcss = []
  for n in range(2, 21):
    kmeans = KMeans(n_clusters=n)
    kmeans.fit(dataset)
    wcss.append(kmeans.inertia_)
  return wcss

n_clusters = list(range(2,21))
somas = soma_quadrados_intra_clusters(dados)
plt.plot(n_clusters, somas, '-o', color='red')
plt.plot([2,20], [somas[0], somas[-1]])
plt.xlabel('Número de clusters')
plt.ylabel("Soma dos quadrados intra-clusters")

def numero_otimo_clusters(wcss):
  x0=1
  y0=wcss[0]
  x1=20
  y1=wcss[-1]

  distancias = []

  for i in range(len(wcss)):
    x = i+2
    y = wcss[i]

    numerador = abs((y1-y0)*x - (x1-x0)*y + x1*y0 - y1*x0)
    denominador = ((y1-y0)**2 + (x1-x0)**2)**0.5
    distancias.append(numerador/denominador)
  print(distancias)
  return distancias.index(max(distancias))+2

k = numero_otimo_clusters(somas)
k